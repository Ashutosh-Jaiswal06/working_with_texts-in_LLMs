# working-with-texts-in-LLMs
This is the basic code to convert raw text data into tokens for training purposes. 

I started with an outdated method and then progress toward modern tokenization known as #Byte Pair Enoding. This encoding method is also used in GPT-2 and GPT-3 architectures.

After this we are moving to the core part of the LLM's working that is predicting one word at a time and why LLM's are autoregressive models. We code how LLM predict next word using input-target pairs and get insights into dataset and dataloader of pytorch.


